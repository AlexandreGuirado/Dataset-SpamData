{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Dataset-SpamData.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOOsmvdCTgurhMSkWFGoq/c",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlexandreGuirado/Dataset-SpamData/blob/main/Dataset_SpamData.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hrnn2xftec-z"
      },
      "source": [
        "#Bibliotecas\n",
        "import random\n",
        "import spacy\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from spacy.util import minibatch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from matplotlib import pyplot as plt\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NP00a_5CetnC"
      },
      "source": [
        "# dados\n",
        "data_path = \"https://raw.githubusercontent.com/AlexandreGuirado/Dataset-SpamData/main/spam.csv\"\n",
        "data = pd.read_csv(data_path, encoding=\"ISO-8859-1\")\n",
        "print(data.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06mTKVo7evhH"
      },
      "source": [
        "data = pd.read_csv(data_path, encoding=\"ISO-8859-1\")\n",
        "observations = len(data.index)\n",
        "print(f\"Tamanho do Dataset: {observations}\\n\")\n",
        "print(data['v1'].value_counts())\n",
        "print()\n",
        "print(data['v1'].value_counts() / len(data.index) * 100.0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bj3Ce6N9eyG3"
      },
      "source": [
        "# Criamos um modelo vazio com o idioma do dataset\n",
        "nlp = spacy.blank(\"en\")\n",
        "\n",
        "# Criamos agora um classificador de texto com classes exclusivas + arquitetura \"bow\" \n",
        "text_cat = nlp.create_pipe(\n",
        "              \"textcat\",\n",
        "              config={\n",
        "                \"exclusive_classes\": True,\n",
        "                \"architecture\": \"bow\"})\n",
        "\n",
        "# Adicionamos o classificador a nosso modelo vazio\n",
        "nlp.add_pipe(text_cat)\n",
        "\n",
        "# Adicionamos as \"classes exclusivas\"\n",
        "text_cat.add_label(\"ham\")\n",
        "text_cat.add_label(\"spam\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mD-mC05Fe28v"
      },
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(\n",
        "      data['v2'], data['v1'], test_size=0.33, random_state=7)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pynbHVqJe7KJ"
      },
      "source": [
        "train_lables = [{'cats': {'ham': label == 'ham',\n",
        "                          'spam': label == 'spam'}}  for label in y_train]\n",
        "\n",
        "test_lables = [{'cats': {'ham': label == 'ham',\n",
        "                      'spam': label == 'spam'}}  for label in y_test]\n",
        "\n",
        "# Spacy model data\n",
        "train_data = list(zip(x_train, train_lables))\n",
        "test_data = list(zip(x_test, test_lables))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zjpef85ae_eP"
      },
      "source": [
        "def train_model(model, train_data, optimizer, batch_size, epochs=10):\n",
        "    losses = {}\n",
        "    random.seed(1)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        random.shuffle(train_data)\n",
        "\n",
        "        batches = minibatch(train_data, size=batch_size)\n",
        "        for batch in batches:\n",
        "            # Split batch into texts and labels\n",
        "            texts, labels = zip(*batch)\n",
        "\n",
        "            # Update model with texts and labels\n",
        "            model.update(texts, labels, sgd=optimizer, losses=losses)\n",
        "        print(\"Loss: {}\".format(losses['textcat']))\n",
        "\n",
        "    return losses['textcat']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hly5ltnffBtC"
      },
      "source": [
        "optimizer = nlp.begin_training()\n",
        "batch_size = 5\n",
        "epochs = 10\n",
        "\n",
        "# Treinar!\n",
        "train_model(nlp, train_data, optimizer, batch_size, epochs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFT7JpgJfI-3"
      },
      "source": [
        "print(train_data[0])\n",
        "sample_test = nlp(train_data[0][0])\n",
        "print(sample_test.cats)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IsBhGgM8vk4X"
      },
      "source": [
        "def get_predictions(model, texts):\n",
        "    # Tokenizar\n",
        "    docs = [model.tokenizer(text) for text in texts]\n",
        "\n",
        "    # textcat para verificar os scores\n",
        "    textcat = model.get_pipe('textcat')\n",
        "    scores, _ = textcat.predict(docs)\n",
        "\n",
        "    # Mostramos os scores mais altos com argmax\n",
        "    predicted_labels = scores.argmax(axis=1)\n",
        "    predicted_class = [textcat.labels[label] for label in predicted_labels]\n",
        "\n",
        "    return predicted_class"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVnr2osbvxJ0"
      },
      "source": [
        "# ACURÁCIA\n",
        "train_predictions = get_predictions(nlp, x_train)\n",
        "test_predictions = get_predictions(nlp, x_test)\n",
        "\n",
        "train_accuracy = accuracy_score(y_train, train_predictions)\n",
        "test_accuracy = accuracy_score(y_test, test_predictions)\n",
        "\n",
        "print(f\"Train accuracy: {train_accuracy}\")\n",
        "print(f\"Test accuracy: {test_accuracy}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFicaCyzwBiR"
      },
      "source": [
        "# MATRIZ DE CONFUSÃO TREINO\n",
        "print(\"TREINO:\")\n",
        "cf_train_matrix = confusion_matrix(y_train, train_predictions)\n",
        "plt.figure(figsize=(10,8))\n",
        "sns.heatmap(cf_train_matrix, annot=True, fmt='d')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKZLLi405r_L"
      },
      "source": [
        "# MATRIZ DE CONFUSÃO TESTE\n",
        "print(\"TESTE:\")\n",
        "cf_test_matrix = confusion_matrix(y_test, test_predictions)\n",
        "plt.figure(figsize=(10,8))\n",
        "sns.heatmap(cf_test_matrix, annot=True, fmt='d')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}